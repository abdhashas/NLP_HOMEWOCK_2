{"cells":[{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","id":"46f519c8","metadata":{"id":"46f519c8"},"source":["#[Your_name]"]},{"cell_type":"code","execution_count":null,"id":"hmYjvSxAyLkF","metadata":{"id":"hmYjvSxAyLkF"},"outputs":[],"source":["# !pip install arabert\n","# !pip install camel-tools\n","# !camel_data -i light\n","# !pip install transformers\n","# !pip uninstall camel-tools\n","# !pip uninstall arabert"]},{"cell_type":"markdown","id":"HIPzLudKv9k8","metadata":{"id":"HIPzLudKv9k8"},"source":["<h1 style=\"text-align: center;\">Sequence Labeling<h1>"]},{"cell_type":"markdown","id":"e7efe140","metadata":{"id":"e7efe140"},"source":["Prepare libraries"]},{"cell_type":"code","execution_count":null,"id":"af6058ed","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1702380289561,"user":{"displayName":"Ola Tabbal","userId":"04260309330816471542"},"user_tz":-180},"id":"af6058ed"},"outputs":[],"source":["# here put every import you need e.g. import nltk\n","# it's better to load what you need from the package by from [] import [] instead of import the whole package\n","import re\n","import nltk\n","from nltk import pos_tag, word_tokenize\n","from nltk.chunk import ne_chunk\n","import pandas as pd\n","from tokenization import tokenize as tk\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","from camel_tools.tokenizers.word import simple_word_tokenize\n","from camel_tools.tagger.default import DefaultTagger\n","from sklearn.linear_model import LogisticRegression\n","# from camel_tools.utils.dediac import dediac_arabic_text\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from nltk.tag import UnigramTagger, BigramTagger\n","import stanza\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from tokenization import tokenize\n","from sklearn import metrics"]},{"cell_type":"markdown","id":"229df102","metadata":{"id":"229df102"},"source":["Download data"]},{"cell_type":"code","execution_count":null,"id":"5504e7ee","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1885,"status":"ok","timestamp":1702380111280,"user":{"displayName":"Ola Tabbal","userId":"04260309330816471542"},"user_tz":-180},"id":"5504e7ee","outputId":"3bd1badb-f09b-4cf0-8bfb-5188fcd7080d"},"outputs":[],"source":["# !wget 'https://drive.google.com/uc?export=download&id=1-3JRPU4aq9t_RRujrJ1HpmAPUbRe0QYK' -O 'IOB.csv'"]},{"cell_type":"markdown","id":"vhRlwHtYv9lH","metadata":{"id":"vhRlwHtYv9lH"},"source":["<h2 dir=\"rtl\">مثال عن كيفية كتابة حلول الطلبات:</h2>"]},{"cell_type":"markdown","id":"8E7xa9XLPW8t","metadata":{"id":"8E7xa9XLPW8t"},"source":["\n","<div dir=\"rtl\">شرح ما يقوم به الكود (like code documentation)<div>"]},{"cell_type":"code","execution_count":null,"id":"d41a9612","metadata":{"id":"d41a9612"},"outputs":[],"source":["# your code here\n"]},{"cell_type":"code","execution_count":null,"id":"aNNrMjaaQanf","metadata":{"id":"aNNrMjaaQanf"},"outputs":[],"source":["# example test"]},{"cell_type":"markdown","id":"d9NfyVxZQXF7","metadata":{"id":"d9NfyVxZQXF7"},"source":["<div dir=\"rtl\">ملاحظاتك في حال وجودها</div>\n","<div dir=\"rtl\">يمكنك إضافة خلايا لكل طلب بقدر ما تشاء، المهم أن تحافظ على تنظيم الملف</div>"]},{"cell_type":"markdown","id":"3400638b","metadata":{"id":"3400638b"},"source":["# Question [1]: Data Preparation"]},{"cell_type":"markdown","id":"2Dsrcp-Av9lO","metadata":{"id":"2Dsrcp-Av9lO"},"source":["## [1.1]"]},{"cell_type":"code","execution_count":null,"id":"fd861638","metadata":{},"outputs":[],"source":["path = 'IOB.csv'\n","\n","def read_file(path,file):\n","    schedule = pd.read_csv(path, names=['word','entity tag','POS','stopword'],sep=',',skiprows=1)\n","    schedule['file'] = file\n","    return schedule\n","\n","\n","schedual = read_file(path,'IOB')\n","schedual.head(5)"]},{"cell_type":"code","execution_count":null,"id":"c20d85ac","metadata":{},"outputs":[],"source":["entity = set(schedual['entity tag'].values)\n","entity"]},{"cell_type":"code","execution_count":null,"id":"imMtzZ7iv9lO","metadata":{"id":"imMtzZ7iv9lO"},"outputs":[],"source":["def lines_to_tuples(words,tags):\n","   \n","    entities = []\n","    for word,tag in zip(words,tags):\n","        my_tuple = (word,tag)\n","        entities.append(my_tuple)\n","\n","    return entities\n","\n","# مثال على استخدام الدالة\n","result = lines_to_tuples(schedual['word'].values,schedual['entity tag'].values)\n","print(result)"]},{"cell_type":"markdown","id":"wzvySosGv9lP","metadata":{"id":"wzvySosGv9lP"},"source":["## [1.2]"]},{"cell_type":"code","execution_count":null,"id":"Oq_wb438v9lP","metadata":{"id":"Oq_wb438v9lP"},"outputs":[],"source":["phrases = []\n","def lines_to_phrases(words,tags,poss,stopwords):\n","    for word,tag,pos,stopword in zip(words,tags,poss,stopwords):\n","        phrase = ' '.join([word,tag,pos,stopword])\n","        phrases.append(phrase)\n","\n","\n","    return phrases\n","\n","result = lines_to_phrases(schedual['word'].values,schedual['entity tag'].values,schedual['POS'].values,schedual['stopword'].values)\n","print(result)\n"]},{"cell_type":"markdown","id":"nh5WzcGtyy2o","metadata":{"id":"nh5WzcGtyy2o"},"source":["## [1.3]"]},{"cell_type":"code","execution_count":null,"id":"IZKWLW28yy27","metadata":{"id":"IZKWLW28yy27"},"outputs":[],"source":["list_of_lists = []\n","def lines_to_list_of_lists(words,tags,poss,stopwords):\n","    my_tuples  = lines_to_tuples(words,tags)\n","    my_phrases = lines_to_phrases(words,tags,poss,stopwords)\n","\n","    for my_tuple,my_phrase in zip(my_tuples,my_phrases):\n","        list_of_lists.append([my_tuple,my_phrase])\n","\n","    return list_of_lists    \n","\n","\n","lines_to_list_of_lists(schedual['word'].values,schedual['entity tag'].values,schedual['POS'].values,schedual['stopword'].values)"]},{"cell_type":"markdown","id":"esmPHkuJyzBd","metadata":{"id":"esmPHkuJyzBd"},"source":["## [1.4]"]},{"cell_type":"code","execution_count":null,"id":"HijKuU0nyzBe","metadata":{"id":"HijKuU0nyzBe"},"outputs":[],"source":["words_list = []\n","tags_list = []\n","\n","def two_lists_word_and_tag(words,tags):\n","    for word,tag in zip(words,tags):\n","        words_list.append(word)\n","        tags_list.append(tag)\n","\n","    return words_list,tags_list \n","\n","\n","my_word_list,my_tag_list = two_lists_word_and_tag(schedual['word'].values,schedual['entity tag'].values)\n","print(my_word_list,my_tag_list)"]},{"cell_type":"markdown","id":"4QOdfsKFyzKt","metadata":{"id":"4QOdfsKFyzKt"},"source":["## [1.5]"]},{"cell_type":"code","execution_count":null,"id":"7zcgCepryzKu","metadata":{"id":"7zcgCepryzKu"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","X = schedual[['word', 'POS', 'stopword']]\n","y = schedual['entity tag']\n","\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1000)\n","\n"]},{"cell_type":"markdown","id":"C7lgaCiYyzQw","metadata":{"id":"C7lgaCiYyzQw"},"source":["## [1.6]"]},{"cell_type":"code","execution_count":null,"id":"51qC3SSgyzQx","metadata":{"id":"51qC3SSgyzQx"},"outputs":[],"source":["# عدد الجمل في كل من الترين والتيست\n","\n","train_data_phrases_len = len(X_train['word'])\n","test_data_phrases_len = len(X_test['word'])\n","print(f'Train phrases length: {train_data_phrases_len}')\n","print(f'Test phrases length: {test_data_phrases_len}')"]},{"cell_type":"markdown","id":"tNFingqGyzVo","metadata":{"id":"tNFingqGyzVo"},"source":["## [1.7]"]},{"cell_type":"code","execution_count":null,"id":"XC1zM6uEyzVp","metadata":{"id":"XC1zM6uEyzVp"},"outputs":[],"source":["def calculate_named_entities_ratio(x_train_data,y_train_data):\n","    words_len = len(x_train_data['word'])\n","    tags_len = 0\n","    for i in y_train_data:\n","        if i == 'I' or i == 'B':\n","            tags_len +=1\n","    \n","    return tags_len/words_len\n","    \n","    \n","    \n","ratio = calculate_named_entities_ratio(X_train,y_train)\n","print(ratio)"]},{"cell_type":"markdown","id":"SoAGtfcry9Ri","metadata":{"id":"SoAGtfcry9Ri"},"source":["## [1.8]"]},{"cell_type":"code","execution_count":null,"id":"Ev5vvt1Fy9Rj","metadata":{"id":"Ev5vvt1Fy9Rj"},"outputs":[],"source":["from farasa.pos import FarasaPOSTagger\n","from farasa.segmenter import FarasaSegmenter\n","\n","arabic_text = \"Tony botros is the best in the whole world\"\n","\n","# Initialize the Farasa Segmenter in Standalone mode\n","farasa_segmenter = FarasaSegmenter(interactive=False)\n","\n","# Tokenize the Arabic text\n","arabic_words = farasa_segmenter.segment(arabic_text)\n","\n","# Initialize the Farasa POS tagger in Standalone mode\n","farasa_pos_tagger = FarasaPOSTagger(interactive=False)\n","\n","# Perform part-of-speech tagging\n","pos_tags = farasa_pos_tagger.tag(arabic_words)\n","\n","print(pos_tags)\n"]},{"cell_type":"markdown","id":"e79aee85","metadata":{},"source":["## [1.9]"]},{"cell_type":"code","execution_count":null,"id":"8127f798","metadata":{},"outputs":[],"source":["count_per_category = y_train.value_counts()\n","\n","most_common_category = count_per_category.idxmax()\n","print(f\"الصنف الأكثر تكرارًا هو: {most_common_category}\")\n"]},{"cell_type":"markdown","id":"2f22713d","metadata":{"id":"2f22713d"},"source":["# Question [2]: Statistics"]},{"cell_type":"code","execution_count":null,"id":"2bfd544d","metadata":{},"outputs":[],"source":["sentence = \"التصلب المتعدد مرض يحتمل أن يسبب إعاقة الدماغ و الحبل النخاعي (الجهاز العصبي المركزي). عند الإصابة بمرض التصلب المتعدد، يهاجم الجهاز المناعي غمد الحماية (المايلين) الذي يغطي الألياف العصبية، ويسبب مشكلات في الاتصال بين الدماغ وبقية الجسم. في النهاية، يمكن أن يسبب تلفاً أو تدهوراً دائما في الألياف العصبية.  \""]},{"cell_type":"code","execution_count":null,"id":"1d5e022b","metadata":{},"outputs":[],"source":["# sentence_tokenize=tk(sentence)\n","sentence_tokenize=tokenize(sentence)"]},{"cell_type":"code","execution_count":null,"id":"3b05d26d","metadata":{},"outputs":[],"source":["comparison_table = {}\n","\n","comparison_table['question_step_number'] = []\n","comparison_table['model_name'] = []\n","comparison_table['features'] = []\n","comparison_table['accuracy'] = []"]},{"cell_type":"markdown","id":"03ef90d1","metadata":{"id":"03ef90d1"},"source":["## [2.1]\n"]},{"cell_type":"code","execution_count":null,"id":"d445898f","metadata":{},"outputs":[],"source":["print(len(schedual[schedual['entity tag']=='O']))"]},{"cell_type":"code","execution_count":null,"id":"acac9432","metadata":{},"outputs":[],"source":["print(len(schedual))"]},{"cell_type":"code","execution_count":null,"id":"674c992d","metadata":{},"outputs":[],"source":["def taget_labet_classifier(words):\n","    return [(word,'O') for word in words]"]},{"cell_type":"code","execution_count":null,"id":"aa32a5fa","metadata":{},"outputs":[],"source":["classified_text = taget_labet_classifier(sentence_tokenize)\n","# classified_text"]},{"cell_type":"code","execution_count":null,"id":"39b72cf0","metadata":{},"outputs":[],"source":["def calculate_accuracy(predicted_labels, true_labels):\n","    correct = sum(pred == true for pred, true in zip(predicted_labels, true_labels))\n","    accuracy = correct / len(predicted_labels) if predicted_labels else 0\n","    return accuracy"]},{"cell_type":"code","execution_count":null,"id":"a8a4181f","metadata":{},"outputs":[],"source":["predicted_labels = taget_labet_classifier(schedual['word'])\n","\n","true_labels = schedual['entity tag'] \n","\n","true_labels_pairs = [(word, tag) for word, tag in zip(schedual['word'], true_labels)]\n","\n","accuracy = calculate_accuracy(predicted_labels, true_labels_pairs)\n","print(f\"accuracy : {accuracy}\")"]},{"cell_type":"markdown","id":"24164796","metadata":{"id":"24164796"},"source":["## [2.2]"]},{"cell_type":"code","execution_count":null,"id":"3ecb7e1f","metadata":{},"outputs":[],"source":["from nltk import FreqDist\n","\n","# word_tokens = word_tokenize(' '.join(X_train['word'].values))\n","word_tokens = tk(' '.join(X_train['word'].values))\n","freq_dist = FreqDist(word_tokens)\n","top_100_words = freq_dist.most_common(100) \n","# print(top_100_words)\n","top_100_words_only = [word for word, _ in top_100_words]\n","# print(top_100_words_only)\n"]},{"cell_type":"code","execution_count":null,"id":"21deb95d","metadata":{},"outputs":[],"source":["filtered_schedual = schedual[schedual['word'].isin(top_100_words_only)].drop_duplicates().dropna()"]},{"cell_type":"code","execution_count":null,"id":"c9449699","metadata":{},"outputs":[],"source":["tagger_lookup = dict(zip(filtered_schedual['word'], filtered_schedual['entity tag']))\n","\n","def classify_words(words):\n","    return [(word, tagger_lookup.get(word,'')) for word in words]"]},{"cell_type":"code","execution_count":null,"id":"ddf6c947","metadata":{},"outputs":[],"source":["classified_text = classify_words(sentence_tokenize)\n","# classified_text"]},{"cell_type":"code","execution_count":null,"id":"cfeecf1d","metadata":{},"outputs":[],"source":["predicted_labels = classify_words(schedual['word'])\n","\n","true_labels = schedual['entity tag'] \n","\n","true_labels_pairs = [(word, tag) for word, tag in zip(schedual['word'], true_labels)]\n","\n","accuracy = calculate_accuracy(predicted_labels, true_labels_pairs)\n","print(f\"accuracy : {accuracy}\")"]},{"cell_type":"code","execution_count":null,"id":"79554614","metadata":{},"outputs":[],"source":["default_tag = 'O'\n","def classify_with_backoff(words):\n","    return [(word, tagger_lookup.get(word, default_tag)) for word in words]"]},{"cell_type":"code","execution_count":null,"id":"5958355d","metadata":{},"outputs":[],"source":["classified_text = classify_with_backoff(sentence_tokenize)\n","# classified_text"]},{"cell_type":"code","execution_count":null,"id":"5eabc6ee","metadata":{},"outputs":[],"source":["predicted_labels = classify_with_backoff(schedual['word'])\n","\n","true_labels = schedual['entity tag'] \n","\n","true_labels_pairs = [(word, tag) for word, tag in zip(schedual['word'], true_labels)]\n","\n","accuracy = calculate_accuracy(predicted_labels, true_labels_pairs)\n","print(f\"accuracy : {accuracy}\")"]},{"cell_type":"markdown","id":"KFerFDFqPwPI","metadata":{"id":"KFerFDFqPwPI"},"source":["## [2.3]"]},{"cell_type":"code","execution_count":null,"id":"c0a64a8d","metadata":{},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(schedual['word'], schedual['entity tag'], test_size=0.2, random_state=1000)\n","train_data=dict(zip(X_train, y_train))\n","test_data=dict(zip(X_test,y_test))\n","tagger_list = [(word, tag) for word, tag in train_data.items()]\n","train_data = [[pair] for pair in tagger_list]\n","tagger_list = [(word, tag) for word, tag in test_data.items()]\n","test_data = [[pair] for pair in tagger_list]"]},{"cell_type":"code","execution_count":null,"id":"zU65GmMbP2Sm","metadata":{"executionInfo":{"elapsed":412,"status":"ok","timestamp":1702380727042,"user":{"displayName":"Ola Tabbal","userId":"04260309330816471542"},"user_tz":-180},"id":"zU65GmMbP2Sm"},"outputs":[],"source":["unigram_tagger = UnigramTagger(train_data)\n","\n","unigram_accuracy = unigram_tagger.accuracy(test_data)\n","print(f\"unigram_accuracy : {unigram_accuracy}\")"]},{"cell_type":"code","execution_count":null,"id":"d144a19b","metadata":{},"outputs":[],"source":["bigram_tagger = BigramTagger(train_data, backoff=unigram_tagger)\n","\n","bigram_accuracy = bigram_tagger.accuracy(test_data)\n","print(f\"bigram_accuracy : {bigram_accuracy}\")"]},{"cell_type":"markdown","id":"oFmJ2HWqRNZa","metadata":{"id":"oFmJ2HWqRNZa"},"source":["# Question [3]: ML Classification"]},{"cell_type":"markdown","id":"cwaCNfS9RNZc","metadata":{"id":"cwaCNfS9RNZc"},"source":["## [3.1]\n"]},{"cell_type":"markdown","id":"xNMH7H1HzfBI","metadata":{"id":"xNMH7H1HzfBI"},"source":["### [3.1.1]"]},{"cell_type":"code","execution_count":null,"id":"LhllhojDRNZd","metadata":{"id":"LhllhojDRNZd"},"outputs":[],"source":["# stanza.download('ar')\n","\n","nlp = stanza.Pipeline('ar', processors='tokenize,pos')\n","def extract_features(sentence, index):\n","    # إعداد محلل اللغة العربية\n","\n","    # تحليل الجملة\n","    doc = nlp(sentence)\n","\n","    features = {}\n","\n","    # سمات صرفية للكلمة الحالية\n","    current_word = doc.sentences[0].words[index - 1].text\n","    features['current_word'] = current_word\n","    current_word_pos = doc.sentences[0].words[index - 1].upos\n","    # features['current_word_pos'] = doc.sentences[0].words[index - 1].upos\n","    # نوع الضمير إذا كانت الكلمة ضميراً\n","    if current_word_pos in ['PRON']:\n","        features['pronoun_type'] = 'subject' if current_word in ['أنا', 'أنت', 'هو', 'هي', 'نحن', 'أنتم', 'هم'] else 'object'\n","    else:\n","        features['pronoun_type'] = 'none'\n","    # المضاف إليه إذا كانت الكلمة اسم إضافة\n","    if current_word_pos in ['ADJ']:\n","        features['adjective'] = 'True'\n","    else:\n","        features['adjective'] = 'False'   \n","\n","    # الجنس والعدد إذا كانت الكلمة اسماً\n","    if current_word_pos in ['NOUN', 'PRON']:\n","        # يمكنك تعديل الشروط والكلمات حسب اللغة العربية\n","        features['gender'] = 'female' if current_word.endswith(\"ة\") else 'male'\n","        features['number'] = 'Feminine Plural' if current_word.endswith(\"ات\") or current_word.endswith(\"يات\") else 'Masculine Plural' if current_word.endswith(\"ون\") or current_word.endswith(\"ين\") else 'Singular'\n","    else:\n","        features['gender'] = 'none'\n","        features['number'] = 'none'\n","\n","\n","    # # سمات صرفية للكلمة السابقة والتالية\n","    # if index > 1:\n","    #     features['previous_word'] = doc.sentences[0].words[index - 2].text\n","    #     features['previous_word_pos'] = doc.sentences[0].words[index - 2].upos\n","\n","    # if index < len(doc.sentences[0].words):\n","    #     features['next_word'] = doc.sentences[0].words[index].text\n","    #     features['next_word_pos'] = doc.sentences[0].words[index].upos\n","\n","    return features, doc, current_word_pos\n","\n","\n","index = 2  \n","word_features,_,_ = extract_features(sentence, index)\n","print(word_features)\n"]},{"cell_type":"markdown","id":"esQLJB9XziyB","metadata":{"id":"esQLJB9XziyB"},"source":["### [3.1.2]"]},{"cell_type":"code","execution_count":null,"id":"PH8I1BTnzj6w","metadata":{"id":"PH8I1BTnzj6w"},"outputs":[],"source":["def features_and_the_word_before_and_after(sentence, index):\n","    features,doc,_ = extract_features(sentence, index)\n","\n","    # سمات صرفية للكلمة السابقة والتالية\n","    if index > 1:\n","        features['previous_word'] = doc.sentences[0].words[index - 2].text\n","        # features['previous_word_pos'] = doc.sentences[0].words[index - 2].upos\n","\n","    if index < len(doc.sentences[0].words):\n","        features['next_word'] = doc.sentences[0].words[index].text\n","        # features['next_word_pos'] = doc.sentences[0].words[index].upos\n","\n","    return features\n","\n","index = 10\n","print(features_and_the_word_before_and_after(sentence,index))\n","    "]},{"cell_type":"markdown","id":"Q2kgxioqzkT9","metadata":{"id":"Q2kgxioqzkT9"},"source":["### [3.1.3]"]},{"cell_type":"code","execution_count":null,"id":"SwBNlGEszl3X","metadata":{"id":"SwBNlGEszl3X"},"outputs":[],"source":["def features_and_pos(sentence,index):\n","    features,_,pos = extract_features(sentence,index)\n","\n","    features['current_word_pos'] = pos\n","\n","    return features\n","\n","index = 10\n","print(features_and_pos(sentence,index))"]},{"cell_type":"markdown","id":"rqy2zGLdzmIm","metadata":{"id":"rqy2zGLdzmIm"},"source":["### [3.1.4]"]},{"cell_type":"code","execution_count":null,"id":"lmI-VkFcznYi","metadata":{"id":"lmI-VkFcznYi"},"outputs":[],"source":["def features_and_the_pos_for_word_before_and_after(sentence, index):\n","    features,doc,_ = extract_features(sentence, index)\n","\n","    # سمات صرفية للكلمة السابقة والتالية\n","    if index > 1:\n","        # features['previous_word'] = doc.sentences[0].words[index - 2].text\n","        features['previous_word_pos'] = doc.sentences[0].words[index - 2].upos\n","\n","    if index < len(doc.sentences[0].words):\n","        # features['next_word'] = doc.sentences[0].words[index].text\n","        features['next_word_pos'] = doc.sentences[0].words[index].upos\n","\n","    return features\n","\n","index = 10\n","print(features_and_the_pos_for_word_before_and_after(sentence,index))\n","    "]},{"cell_type":"markdown","id":"9b3Bhi2Uzns2","metadata":{"id":"9b3Bhi2Uzns2"},"source":["### [3.1.5]"]},{"cell_type":"code","execution_count":null,"id":"3qRtfrJTzpQY","metadata":{"id":"3qRtfrJTzpQY"},"outputs":[],"source":["def features_bigram(sentence,index):\n","    features,doc,pos = extract_features(sentence, index)\n","\n","    # Previous word features\n","    if index > 0:\n","        prev_word = doc.sentences[0].words[index - 2].text\n","        prev_word_pos = doc.sentences[0].words[index - 2].upos\n","        features['prev_word'] = prev_word\n","        features['prev_word_pos'] = prev_word_pos\n","\n","        # Bigram features for POS\n","        features['pos_bigram_prev_current'] = f\"{prev_word_pos}_{pos}\"\n","\n","    # Next word features\n","    if index < len(doc.sentences[0].words) - 1:\n","        next_word = doc.sentences[0].words[index].text\n","        next_word_pos = doc.sentences[0].words[index].upos\n","        features['next_word'] = next_word\n","        features['next_word_pos'] = next_word_pos\n","\n","        # Bigram features for POS\n","        features['pos_bigram_current_next'] = f\"{pos}_{next_word_pos}\"\n","\n","\n","    return features    \n","\n","\n","index = 10\n","print(features_bigram(sentence,index))\n","\n","    "]},{"cell_type":"markdown","id":"XDjHC6-czpmO","metadata":{"id":"XDjHC6-czpmO"},"source":["### [3.1.6]"]},{"cell_type":"code","execution_count":null,"id":"wOqBTCf5zrBA","metadata":{"id":"wOqBTCf5zrBA"},"outputs":[],"source":["def features_and_prev_pos(sentence,index):\n","    features,doc,_ = extract_features(sentence, index)\n","    if index > 1:\n","        features['previous_word_pos'] = doc.sentences[0].words[index - 2].upos\n","\n","    return features\n","\n","\n","index = 10\n","print(features_and_prev_pos(sentence,index))\n"]},{"cell_type":"markdown","id":"H6yVZDx9zrQF","metadata":{"id":"H6yVZDx9zrQF"},"source":["### [3.1.7]"]},{"cell_type":"code","execution_count":null,"id":"bcfGNYc6zsTh","metadata":{"id":"bcfGNYc6zsTh"},"outputs":[],"source":["def features_and_new_features(sentence,index):\n","    features,doc,_ = extract_features(sentence, index)\n","\n","    current_word_form =  doc.sentences[0].words[index - 1].feats\n","\n","    features['current_word_form'] = current_word_form\n","\n","    return features\n","\n","\n","index = 10\n","print(features_and_new_features(sentence,index))\n","\n","# \"\"\" \n","# index 10 = 'الحبل'\n","# \"Case=Gen\": الكلمة في حالة الجر (Genitive).\n","# \"Definite=Def\": الكلمة محددة (Definite).\n","# \"Number=Sing\": الكلمة في صيغة المفرد (Singular).\n","# \"\"\"\n"]},{"cell_type":"markdown","id":"MbJeF_CxRNZd","metadata":{"id":"MbJeF_CxRNZd"},"source":["## [3.2]"]},{"cell_type":"code","execution_count":36,"id":"7fb0e241","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 1.00\n"]}],"source":["def bayes_naive_multinomial(sentence, method,features_type,word_pos=False):\n","    tokenized_sentence = tokenize(sentence)\n","    all_Words = []\n","    words_pos = []\n","    # Extract features for all words in the sentence\n","    for i in range(len(tokenized_sentence)):\n","        features, _, pos = method(sentence, i)\n","        all_Words.append(features)\n","        if word_pos == True:\n","            words_pos.append(pos)\n","\n","\n","    if word_pos == True:\n","        y_all = [item for item in words_pos]\n","        X =  [item['current_word'] for item in all_Words]\n","    \n","        # Split the data into training and testing sets\n","        X_train, X_test, y_train, y_test = train_test_split(X, y_all, test_size=0.25, random_state=42)\n","\n","        # Extract features and labels for training set\n","        X_train_features = [item for item in X_train]\n","        X_test_features = [item for item in X_test]\n","\n","        # Create a CountVectorizer and transform the training and testing features\n","        vectorizer = CountVectorizer()\n","        X_train_vec = vectorizer.fit_transform(X_train_features)\n","        X_test_vec = vectorizer.transform(X_test_features)\n","\n","        # Create and train the Naive Bayes classifier\n","        nb_classifier = MultinomialNB()\n","        nb_classifier.fit(X_train_vec, y_train)\n","\n","        # Make predictions on the testing set\n","        y_pred = nb_classifier.predict(X_test_vec)\n","\n","        # Evaluate the performance of the classifier\n","        accuracy = metrics.accuracy_score(y_test, y_pred)\n","        print(f\"Accuracy: {accuracy:.2f}\")\n","\n","\n","    else:\n","    # Extract the labels (pronoun_type) from the features\n","        y_all = [item[features_type] for item in all_Words]\n","        X =  [item['current_word'] for item in all_Words]\n","\n","        \n","        # Split the data into training and testing sets\n","        X_train, X_test, y_train, y_test = train_test_split(X, y_all, test_size=0.25, random_state=42)\n","\n","        # Extract features and labels for training set\n","        X_train_features = [item for item in X_train]\n","        X_test_features = [item for item in X_test]\n","\n","        # Create a CountVectorizer and transform the training and testing features\n","        vectorizer = CountVectorizer()\n","        X_train_vec = vectorizer.fit_transform(X_train_features)\n","        X_test_vec = vectorizer.transform(X_test_features)\n","\n","        # Create and train the Naive Bayes classifier\n","        nb_classifier = MultinomialNB()\n","        nb_classifier.fit(X_train_vec, y_train)\n","\n","        # Make predictions on the testing set\n","        y_pred = nb_classifier.predict(X_test_vec)\n","\n","        # Evaluate the performance of the classifier\n","        accuracy = metrics.accuracy_score(y_test, y_pred)\n","        print(f\"Accuracy: {accuracy:.2f}\")\n","\n","# Example usage\n","bayes_naive_multinomial(sentence, extract_features,'pronoun_type')\n"]},{"cell_type":"code","execution_count":33,"id":"794d54b8","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.40\n"]}],"source":["# pos\n","bayes_naive_multinomial(sentence, extract_features,'pos',word_pos=True)\n"]},{"cell_type":"code","execution_count":37,"id":"580e4ccf","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.93\n"]}],"source":["# adjective\n","bayes_naive_multinomial(sentence, extract_features,'adjective')\n"]},{"cell_type":"code","execution_count":38,"id":"bc892e3c","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.80\n"]}],"source":["# gender\n","bayes_naive_multinomial(sentence, extract_features,'gender')\n"]},{"cell_type":"code","execution_count":39,"id":"9e0d112a","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.80\n"]}],"source":["# number\n","bayes_naive_multinomial(sentence, extract_features,'number')\n"," \n"]},{"cell_type":"markdown","id":"GY-HhD2YRNZe","metadata":{"id":"GY-HhD2YRNZe"},"source":["## [3.3]"]},{"cell_type":"code","execution_count":40,"id":"l-5TSuHWRNZe","metadata":{"id":"l-5TSuHWRNZe"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.93\n"]}],"source":["def multinomial_logistic_regression(sentence, method,features_type,word_pos=False):\n","    tokenized_sentence = tokenize(sentence)\n","    all_Words = []\n","    words_pos = []\n","    # Extract features for all words in the sentence\n","    for i in range(len(tokenized_sentence)):\n","        features, _, pos = method(sentence, i)\n","        all_Words.append(features)\n","        if word_pos:\n","            words_pos.append(pos)\n","\n","\n","    if words_pos:\n","        # Extract the labels (pronoun_type) from the features\n","        y_all = [item for item in words_pos]\n","        X =  [item['current_word'] for item in all_Words]\n","\n","        # Split the data into training and testing sets\n","        X_train, X_test, y_train, y_test = train_test_split(X, y_all, test_size=0.25, random_state=42)\n","\n","        X_train = [doc for doc in X_train if doc]\n","        X_test = [doc for doc in X_test if doc]\n","        \n","        # Extract features and labels for training set\n","        X_train_features = [item for item in X_train]\n","        X_test_features = [item for item in X_test]\n","\n","\n","        # Create a CountVectorizer and transform the training and testing features\n","        vectorizer = CountVectorizer()\n","        X_train_vec = vectorizer.fit_transform(X_train_features)\n","        X_test_vec = vectorizer.transform(X_test_features)\n","\n","        # Create and train the Naive Bayes classifier\n","        logistic_classifier = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n","        logistic_classifier.fit(X_train_vec, y_train)\n","\n","\n","        # Make predictions on the testing set\n","        y_pred = logistic_classifier.predict(X_test_vec)\n","\n","        # Evaluate the performance of the classifier\n","        accuracy = metrics.accuracy_score(y_test, y_pred)\n","        print(f\"Accuracy: {accuracy:.2f}\")        \n","\n","    else:\n","        # Extract the labels (pronoun_type) from the features\n","        y_all = [item[features_type] for item in all_Words]\n","        X =  [item['current_word'] for item in all_Words]\n","\n","        # Split the data into training and testing sets\n","        X_train, X_test, y_train, y_test = train_test_split(X, y_all, test_size=0.25, random_state=42)\n","\n","        X_train = [doc for doc in X_train if doc]\n","        X_test = [doc for doc in X_test if doc]\n","        \n","        # Extract features and labels for training set\n","        X_train_features = [item for item in X_train]\n","        X_test_features = [item for item in X_test]\n","\n","\n","        # Create a CountVectorizer and transform the training and testing features\n","        vectorizer = CountVectorizer()\n","        X_train_vec = vectorizer.fit_transform(X_train_features)\n","        X_test_vec = vectorizer.transform(X_test_features)\n","\n","\n","        # Create and train the Naive Bayes classifier\n","        logistic_classifier = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n","        logistic_classifier.fit(X_train_vec, y_train)\n","\n","\n","        # Make predictions on the testing set\n","        y_pred = logistic_classifier.predict(X_test_vec)\n","\n","\n","        # Evaluate the performance of the classifier\n","        accuracy = metrics.accuracy_score(y_test, y_pred)\n","        print(f\"Accuracy: {accuracy:.2f}\")\n","\n","# Example usage\n","multinomial_logistic_regression(sentence, extract_features,'adjective')\n"]},{"cell_type":"code","execution_count":41,"id":"a0b70f0b","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.40\n"]}],"source":["# pos\n","multinomial_logistic_regression(sentence, extract_features,'pos',word_pos=True)\n"]},{"cell_type":"code","execution_count":42,"id":"ab5dce18","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.67\n"]}],"source":["# gender\n","multinomial_logistic_regression(sentence, extract_features,'gender')\n"]},{"cell_type":"code","execution_count":43,"id":"84bac687","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.80\n"]}],"source":["# number\n","bayes_naive_multinomial(sentence, extract_features,'number')\n"," "]},{"cell_type":"markdown","id":"KV9D86kbRNZe","metadata":{"id":"KV9D86kbRNZe"},"source":["## [3.4]"]},{"cell_type":"code","execution_count":null,"id":"e220cd3e","metadata":{},"outputs":[],"source":["from sklearn_crfsuite import CRF\n","\n","def crf_model(sentence, method,features_type,word_pos=False):\n","    tokenized_sentence = tokenize(sentence)\n","    all_Words = []\n","    words_pos = []\n","    # Extract features for all words in the sentence\n","    for i in range(len(tokenized_sentence)):\n","        features, _, pos = method(sentence, i)\n","        all_Words.append(features)\n","        if word_pos:\n","            words_pos.append(pos)\n","\n","\n","    if words_pos:\n","        y_all = [item for item in words_pos]\n","        X = [{'word': item['current_word']} for item in all_Words]\n","        \n","        X_train, X_test, y_train, y_test = train_test_split(X, y_all, test_size=0.25, random_state=42)\n","\n","        # X_train = [doc for doc in X_train if doc]\n","        # X_test = [doc for doc in X_test if doc]\n","        \n","        # # Extract features and labels for training set\n","        # X_train_features = [item for item in X_train]\n","        # X_test_features = [item for item in X_test]\n","\n","        # X_train_crf = [dict(zip(item, item)) for item in X_train]\n","        # X_test_crf = [dict(zip(item, item)) for item in X_test]\n","\n","        print(X_train)\n","        print(y_train)\n","\n","        crf = CRF()\n","    \n","        try:\n","            crf.fit(X_train, y_train)\n","        except Exception as e:\n","            print(f\"Error during model training: {e}\")\n","            return\n","\n","\n","        # Make predictions on the testing set\n","        y_pred = crf.predict(X_train)\n","        print(len(X_train))\n","        print(len(y_test))\n","        print(len(y_pred))\n","\n","        # Evaluate the performance of the classifier\n","        accuracy = metrics.accuracy_score(y_test, y_pred)\n","        print(f\"Accuracy: {accuracy:.2f}\")        \n","\n","    else:\n","        # Extract the labels (pronoun_type) from the features\n","        y_all = [item[features_type] for item in all_Words]\n","        \n","        # Split the data into training and testing sets\n","        X_train, X_test, y_train, y_test = train_test_split(all_Words, y_all, test_size=0.25, random_state=42)\n","\n","        X_train = [doc for doc in X_train if doc]\n","        X_test = [doc for doc in X_test if doc]\n","        \n","        # Extract features and labels for training set\n","        X_train_features = [item[features_type] for item in X_train]\n","        X_test_features = [item[features_type] for item in X_test]\n","\n","\n","        # Create a CountVectorizer and transform the training and testing features\n","        vectorizer = CountVectorizer()\n","        X_train_vec = vectorizer.fit_transform(X_train_features)\n","        X_test_vec = vectorizer.transform(X_test_features)\n","\n","\n","        # Create and train the Naive Bayes classifier\n","        logistic_classifier = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n","        logistic_classifier.fit(X_train_vec, y_train)\n","\n","\n","        # Make predictions on the testing set\n","        y_pred = logistic_classifier.predict(X_test_vec)\n","\n","        # Evaluate the performance of the classifier\n","        accuracy = metrics.accuracy_score(y_test, y_pred)\n","        print(f\"Accuracy: {accuracy:.2f}\")\n","\n","# Example usage\n","crf_model(sentence, extract_features,'pos',word_pos=True)\n"]},{"cell_type":"code","execution_count":null,"id":"906a3e95","metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn_crfsuite import CRF\n","from sklearn.metrics import classification_report\n","\n","# Sample data for part-of-speech tagging\n","data = [\n","    {'word': 'The', 'pos': 'DET'},\n","    {'word': 'quick', 'pos': 'ADJ'},\n","    {'word': 'brown', 'pos': 'ADJ'},\n","    {'word': 'fox', 'pos': 'NOUN'},\n","    {'word': 'jumps', 'pos': 'VERB'},\n","    {'word': 'over', 'pos': 'ADP'},\n","    {'word': 'the', 'pos': 'DET'},\n","    {'word': 'lazy', 'pos': 'ADJ'},\n","    {'word': 'dog', 'pos': 'NOUN'},\n","    {'word': '.', 'pos': 'PUNCT'},\n","]\n","\n","# Modified feature extraction to include previous and next words\n","def word_features(sentence, i):\n","    features = {\n","        'word': sentence[i]['word'],\n","        'prev_word': '' if i == 0 else sentence[i - 1]['word'],\n","        'next_word': '' if i == len(sentence) - 1 else sentence[i + 1]['word'],\n","    }\n","    return features\n","\n","# Extract features and labels\n","X = [word_features(data, i) for i in range(len(data))]\n","y = [item['pos'] for item in data]\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n","\n","# Displaying the example data\n","print(\"Example X_train:\")\n","for x in X_train:\n","    print(x)\n","\n","print(\"\\nExample y_train:\")\n","print(y_train)\n","\n","# Create and train the CRF model\n","crf = CRF()\n","crf.fit(X_train, y_train)\n","\n","# Make predictions on the testing set\n","y_pred = crf.predict(X_test)\n","\n","# Evaluate the performance of the classifier\n","report = classification_report(y_test, y_pred)\n","print(\"Classification Report:\\n\", report)\n"]},{"cell_type":"markdown","id":"3ztGb8ulv9lY","metadata":{"id":"3ztGb8ulv9lY"},"source":["## [3.5]"]},{"cell_type":"code","execution_count":49,"id":"nmtgekQyv9ll","metadata":{"id":"nmtgekQyv9ll"},"outputs":[{"name":"stderr","output_type":"stream","text":["MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n","https://github.com/hmmlearn/hmmlearn/issues/335\n","https://github.com/hmmlearn/hmmlearn/issues/340\n"]},{"name":"stdout","output_type":"stream","text":["HMM Accuracy: 0.00\n"]}],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer\n","from hmmlearn import hmm\n","from sklearn import metrics\n","\n","def my_hmm(sentence, method, features_type, word_pos=False):\n","    tokenized_sentence = tokenize(sentence)\n","    all_Words = []\n","    words_pos = []\n","\n","    # Extract features for all words in the sentence\n","    for i in range(len(tokenized_sentence)):\n","        features, _, pos = method(sentence, i)\n","        all_Words.append(features)\n","        if word_pos:\n","            words_pos.append(pos)\n","\n","    if words_pos:\n","        # Extract the labels (pronoun_type) from the features\n","        y_all = [item for item in words_pos]\n","        X = [item['current_word'] for item in all_Words]\n","\n","        # Split the data into training and testing sets\n","        X_train, X_test, y_train, y_test = train_test_split(X, y_all, test_size=0.25, random_state=42)\n","\n","        X_train = [doc for doc in X_train if doc]\n","        X_test = [doc for doc in X_test if doc]\n","\n","        # Extract features and labels for the training set\n","        X_train_features = [item for item in X_train]\n","        X_test_features = [item for item in X_test]\n","\n","        # Create a CountVectorizer and transform the training and testing features\n","        vectorizer = CountVectorizer()\n","        X_train_vec = vectorizer.fit_transform(X_train_features).toarray()\n","        X_test_vec = vectorizer.transform(X_test_features).toarray()\n","\n","        # Now, let's create an HMM model\n","        # Assuming X_train_vec and X_test_vec are the features for the HMM model\n","        my_hmm_model = hmm.MultinomialHMM(n_components=3, n_iter=100)  # You can adjust the number of states and iterations\n","        my_hmm_model.fit(X_train_vec, lengths=[len(X_train_vec)])\n","\n","        # Make predictions on the testing set using HMM\n","        _, hmm_pred = my_hmm_model.decode(X_test_vec, algorithm=\"viterbi\")\n","\n","        # Evaluate the performance of the HMM model\n","        hmm_accuracy = metrics.accuracy_score(y_test, hmm_pred)\n","        print(f\"HMM Accuracy: {hmm_accuracy:.2f}\")\n","\n","# Example usage\n","my_hmm(sentence, extract_features, 'pos', word_pos=True)\n"]},{"cell_type":"markdown","id":"Jp-C0ZwrS8AY","metadata":{"id":"Jp-C0ZwrS8AY"},"source":["# Question [4]: DL Classification"]},{"cell_type":"markdown","id":"pwtVy8DES8AZ","metadata":{"id":"pwtVy8DES8AZ"},"source":["## [4.1]\n"]},{"cell_type":"code","execution_count":null,"id":"HRz-DuqWv9lv","metadata":{"id":"HRz-DuqWv9lv"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"npObP2JLUBgw","metadata":{"id":"npObP2JLUBgw"},"source":["## [4.2]"]},{"cell_type":"code","execution_count":null,"id":"V_KxmQdgUBg5","metadata":{"id":"V_KxmQdgUBg5"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"pTIrTECWUBg5","metadata":{"id":"pTIrTECWUBg5"},"source":["## [4.3]"]},{"cell_type":"code","execution_count":null,"id":"OqmCKe5lUBg6","metadata":{"id":"OqmCKe5lUBg6"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"_D24_lfNv9ly","metadata":{"id":"_D24_lfNv9ly"},"source":["The Comparison Dictionary example"]},{"cell_type":"code","execution_count":null,"id":"hZ7kAPp_v9ly","metadata":{"id":"hZ7kAPp_v9ly"},"outputs":[],"source":["comparison_table = {}\n","\n","comparison_table['question_step_number'] = []\n","comparison_table['model_name'] = []\n","comparison_table['features'] = []\n","comparison_table['accuracy'] = []"]},{"cell_type":"code","execution_count":null,"id":"3GSEj4Zfv9mD","metadata":{"id":"3GSEj4Zfv9mD"},"outputs":[],"source":["import pandas as pd\n","\n","df = pd.DataFrame(comparison_table)\n","df"]},{"cell_type":"code","execution_count":null,"id":"Ej1zS1jVv9mE","metadata":{"id":"Ej1zS1jVv9mE"},"outputs":[],"source":["df.to_csv(\"[your_name].csv\", index=False)"]},{"cell_type":"markdown","id":"hvAOY1Lf0r8j","metadata":{"id":"hvAOY1Lf0r8j"},"source":["<h1 style=\"text-align: center;\">Sequence to Sequence<h1>"]},{"cell_type":"markdown","id":"ywKh0_NO08hh","metadata":{"id":"ywKh0_NO08hh"},"source":["Prepare libraries"]},{"cell_type":"code","execution_count":null,"id":"a5yqXZOq08h7","metadata":{"id":"a5yqXZOq08h7"},"outputs":[],"source":["# here put every import you need e.g. import nltk\n","# it's better to load what you need from the package by from [] import [] instead of import the whole package\n","!pip install arabert"]},{"cell_type":"markdown","id":"6ZlVD7Z608h8","metadata":{"id":"6ZlVD7Z608h8"},"source":["Download data"]},{"cell_type":"code","execution_count":null,"id":"pb6H0pmU08h8","metadata":{"id":"pb6H0pmU08h8"},"outputs":[],"source":["# !wget 'https://drive.google.com/uc?export=download&id=1OoUGXNKnmm3KYabreBFWMV9mr2JwQOJH' -O 'test.csv'\n","# !wget 'https://drive.google.com/uc?export=download&id=1xnw6kKitwQwYgaOAQppDbj6T3ARCwRqz' -O 'train.csv'"]},{"cell_type":"code","execution_count":null,"id":"5a4554db","metadata":{},"outputs":[],"source":["from arabert import ArabertPreprocessor\n","from arabert.aragpt2.grover.modeling_gpt2 import GPT2LMHeadModel"]},{"cell_type":"markdown","id":"0YB5bMys1Xum","metadata":{"id":"0YB5bMys1Xum"},"source":["# [Extra 1]"]},{"cell_type":"code","execution_count":null,"id":"-DOOfEuG1aWF","metadata":{"id":"-DOOfEuG1aWF"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"Xo1w5eJK1esR","metadata":{"id":"Xo1w5eJK1esR"},"source":["# [2]"]},{"cell_type":"code","execution_count":null,"id":"Ax0W2IAM1f4X","metadata":{"id":"Ax0W2IAM1f4X"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"iUravYPX1gFn","metadata":{"id":"iUravYPX1gFn"},"source":["# [3]"]},{"cell_type":"code","execution_count":null,"id":"ni8b0PaW1hIl","metadata":{"id":"ni8b0PaW1hIl"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"}},"nbformat":4,"nbformat_minor":5}
